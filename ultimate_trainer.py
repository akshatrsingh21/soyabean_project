import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_selection import SelectKBest, f_classif, RFECV
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import warnings
warnings.filterwarnings('ignore')

class UltimateSoybeanTrainer:
    def __init__(self, excel_path):
        self.excel_path = excel_path
        self.data = None
        self.best_model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_selector = None
        
    def load_and_enhance_data(self):
        """Load data and apply data augmentation"""
        print("Loading and enhancing data...")
        self.data = pd.read_excel(self.excel_path)
        print(f"Original shape: {self.data.shape}")
        
        # Clean the data
        self.data = self.data.dropna(axis=1, how='all')
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        self.data[numeric_cols] = self.data[numeric_cols].fillna(0)
        self.data = self.data.dropna(subset=['Variety', 'Average R', 'Average G', 'Average B'])
        
        print(f"Cleaned shape: {self.data.shape}")
        print("Original variety distribution:")
        print(self.data['Variety'].value_counts())
        
        # Apply data augmentation to balance the dataset
        self.augment_data()
        
        print("Final variety distribution after augmentation:")
        print(self.data['Variety'].value_counts())
        
        return self.data
    
    def augment_data(self):
        """Augment data to balance classes and increase dataset size"""
        augmented_data = []
        
        for variety in self.data['Variety'].unique():
            variety_data = self.data[self.data['Variety'] == variety]
            n_samples = len(variety_data)
            
            # Target 100 samples per class for balanced dataset
            target_samples = 100
            if n_samples < target_samples:
                # Create synthetic samples
                for _ in range(target_samples - n_samples):
                    sample = variety_data.sample(1).iloc[0]
                    new_sample = sample.copy()
                    
                    # Add noise to RGB values (5% variation)
                    noise_factor = 0.05
                    new_sample['Average R'] *= (1 + np.random.uniform(-noise_factor, noise_factor))
                    new_sample['Average G'] *= (1 + np.random.uniform(-noise_factor, noise_factor))
                    new_sample['Average B'] *= (1 + np.random.uniform(-noise_factor, noise_factor))
                    
                    augmented_data.append(new_sample)
        
        # Add augmented data to original dataset
        if augmented_data:
            augmented_df = pd.DataFrame(augmented_data)
            self.data = pd.concat([self.data, augmented_df], ignore_index=True)
    
    def create_advanced_features(self):
        """Create comprehensive feature set"""
        print("Creating advanced features...")
        
        features = self.data[['Average R', 'Average G', 'Average B']].copy()
        
        # Basic features
        features['Brightness'] = (features['Average R'] + features['Average G'] + features['Average B']) / 3
        features['R_G_Ratio'] = features['Average R'] / features['Average G'].replace(0, 1)
        features['R_B_Ratio'] = features['Average R'] / features['Average B'].replace(0, 1)
        features['G_B_Ratio'] = features['Average G'] / features['Average B'].replace(0, 1)
        
        # Color space transformations
        features['HSV_H'] = self.rgb_to_hsv(features['Average R'], features['Average G'], features['Average B'])[0]
        features['HSV_S'] = self.rgb_to_hsv(features['Average R'], features['Average G'], features['Average B'])[1]
        features['HSV_V'] = self.rgb_to_hsv(features['Average R'], features['Average G'], features['Average B'])[2]
        
        # Advanced statistical features
        features['RGB_Std'] = features[['Average R', 'Average G', 'Average B']].std(axis=1)
        features['RGB_Range'] = features[['Average R', 'Average G', 'Average B']].max(axis=1) - features[['Average R', 'Average G', 'Average B']].min(axis=1)
        features['RGB_CV'] = features['RGB_Std'] / features['Brightness']  # Coefficient of variation
        
        # Color dominance features
        features['Red_Dominance'] = features['Average R'] / features[['Average G', 'Average B']].max(axis=1)
        features['Green_Dominance'] = features['Average G'] / features[['Average R', 'Average B']].max(axis=1)
        features['Blue_Dominance'] = features['Average B'] / features[['Average R', 'Average G']].max(axis=1)
        
        # Advanced ratios and proportions
        total_color = features['Average R'] + features['Average G'] + features['Average B']
        features['R_Proportion'] = features['Average R'] / total_color
        features['G_Proportion'] = features['Average G'] / total_color
        features['B_Proportion'] = features['Average B'] / total_color
        
        # Quadratic and interaction features
        features['R_Squared'] = features['Average R'] ** 2
        features['G_Squared'] = features['Average G'] ** 2
        features['B_Squared'] = features['Average B'] ** 2
        features['R_G_Interaction'] = features['Average R'] * features['Average G']
        features['R_B_Interaction'] = features['Average R'] * features['Average B']
        features['G_B_Interaction'] = features['Average G'] * features['Average B']
        
        # Log transformations
        features['Log_R'] = np.log1p(features['Average R'])
        features['Log_G'] = np.log1p(features['Average G'])
        features['Log_B'] = np.log1p(features['Average B'])
        
        print(f"Created {features.shape[1]} advanced features")
        return features
    
    def rgb_to_hsv(self, r, g, b):
        """Convert RGB to HSV manually"""
        r, g, b = r/255.0, g/255.0, b/255.0
        mx = np.maximum.reduce([r, g, b])
        mn = np.minimum.reduce([r, g, b])
        df = mx - mn
        
        # Hue calculation
        if mx == mn:
            h = 0
        elif mx == r:
            h = (60 * ((g - b) / df) + 360) % 360
        elif mx == g:
            h = (60 * ((b - r) / df) + 120) % 360
        elif mx == b:
            h = (60 * ((r - g) / df) + 240) % 360
        
        # Saturation calculation
        if mx == 0:
            s = 0
        else:
            s = (df / mx) * 100
        
        # Value calculation
        v = mx * 100
        
        return h, s, v
    
    def feature_engineering(self, X, y):
        """Advanced feature selection"""
        print("Performing advanced feature selection...")
        
        # Use recursive feature elimination
        estimator = RandomForestClassifier(n_estimators=100, random_state=42)
        selector = RFECV(estimator, step=1, cv=5, scoring='accuracy', n_jobs=-1)
        X_selected = selector.fit_transform(X, y)
        self.feature_selector = selector
        
        selected_features = X.columns[selector.support_]
        print(f"Selected {len(selected_features)} best features:")
        for i, feature in enumerate(selected_features, 1):
            print(f"{i}. {feature}")
        
        return X_selected, selected_features
    
    def train_ultimate_model(self):
        """Train ensemble model with advanced techniques"""
        # Prepare features and labels
        X = self.create_advanced_features()
        y = self.data['Variety'].astype(str)
        
        # Encode labels
        y_encoded = self.label_encoder.fit_transform(y)
        
        # Feature selection
        X_selected, selected_features = self.feature_engineering(X, y_encoded)
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X_selected)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        print(f"Training samples: {X_train.shape[0]}")
        print(f"Testing samples: {X_test.shape[0]}")
        
        # Define base models
        models = {
            'XGBoost': XGBClassifier(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                subsample=0.8, colsample_bytree=0.8, random_state=42
            ),
            'LightGBM': LGBMClassifier(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                num_leaves=31, random_state=42
            ),
            'RandomForest': RandomForestClassifier(
                n_estimators=200, max_depth=10, min_samples_split=5,
                min_samples_leaf=2, random_state=42, class_weight='balanced'
            ),
            'SVM': SVC(
                C=1.0, kernel='rbf', gamma='scale', probability=True, random_state=42
            )
        }
        
        # Train individual models
        individual_results = {}
        for name, model in models.items():
            print(f"Training {name}...")
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            individual_results[name] = accuracy
            print(f"{name} Accuracy: {accuracy:.3f}")
        
        # Create ensemble model
        print("\nCreating ensemble model...")
        ensemble = VotingClassifier(
            estimators=[
                ('xgb', models['XGBoost']),
                ('lgbm', models['LightGBM']),
                ('rf', models['RandomForest']),
                ('svm', models['SVM'])
            ],
            voting='soft',
            n_jobs=-1
        )
        
        # Train ensemble
        ensemble.fit(X_train, y_train)
        y_pred_ensemble = ensemble.predict(X_test)
        ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)
        
        print(f"\nEnsemble Model Accuracy: {ensemble_accuracy:.3f}")
        print("\nClassification Report:")
        class_names = [str(cls) for cls in self.label_encoder.classes_]
        print(classification_report(y_test, y_pred_ensemble, target_names=class_names))
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred_ensemble)
        self.plot_confusion_matrix(cm, class_names)
        
        # Cross-validation
        print("\nPerforming cross-validation...")
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(ensemble, X_scaled, y_encoded, cv=cv, scoring='accuracy')
        print(f"Cross-validation scores: {cv_scores}")
        print(f"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
        
        self.best_model = ensemble
        return ensemble_accuracy
    
    def plot_confusion_matrix(self, cm, class_names):
        """Plot confusion matrix"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=class_names, yticklabels=class_names)
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        print("Confusion matrix saved as 'confusion_matrix.png'")
    
    def save_model(self):
        """Save the ultimate model"""
        joblib.dump({
            'model': self.best_model,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_selector': self.feature_selector,
            'accuracy': self.train_ultimate_model()
        }, 'soybean_ultimate_model.pkl')
        print("Ultimate model saved as 'soybean_ultimate_model.pkl'")
    
    def create_detailed_analysis(self):
        """Create comprehensive analysis report"""
        print("\nCreating detailed analysis report...")
        
        plt.figure(figsize=(15, 10))
        
        # 1. Feature importance
        if hasattr(self.best_model, 'feature_importances_'):
            plt.subplot(2, 2, 1)
            feature_importance = self.best_model.feature_importances_
            top_indices = np.argsort(feature_importance)[-10:][::-1]
            top_features = self.create_advanced_features().columns[top_indices]
            top_importance = feature_importance[top_indices]
            
            plt.barh(range(len(top_importance)), top_importance)
            plt.yticks(range(len(top_importance)), top_features)
            plt.xlabel('Importance')
            plt.title('Top 10 Feature Importance')
        
        # 2. Class distribution
        plt.subplot(2, 2, 2)
        variety_counts = self.data['Variety'].value_counts()
        plt.bar(variety_counts.index.astype(str), variety_counts.values)
        plt.title('Class Distribution After Augmentation')
        plt.xlabel('Variety')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        
        # 3. Correlation heatmap
        plt.subplot(2, 2, 3)
        correlation_matrix = self.create_advanced_features().corr()
        sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)
        plt.title('Feature Correlation Heatmap')
        
        # 4. Accuracy comparison
        plt.subplot(2, 2, 4)
        models = ['XGBoost', 'LightGBM', 'RandomForest', 'SVM', 'Ensemble']
        # Placeholder for accuracy values - you would need to track these
        accuracies = [0.7, 0.72, 0.68, 0.65, 0.85]  # Example values
        plt.bar(models, accuracies)
        plt.title('Model Accuracy Comparison')
        plt.ylabel('Accuracy')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig('detailed_analysis.png', dpi=300, bbox_inches='tight')
        print("Detailed analysis saved as 'detailed_analysis.png'")

def main():
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python ultimate_trainer.py <path_to_excel_file>")
        return
    
    excel_path = sys.argv[1]
    
    # Initialize ultimate trainer
    trainer = UltimateSoybeanTrainer(excel_path)
    
    # Load and enhance data
    data = trainer.load_and_enhance_data()
    
    # Train ultimate model
    accuracy = trainer.train_ultimate_model()
    
    if accuracy is not None:
        # Save model
        trainer.save_model()
        
        # Create detailed analysis
        trainer.create_detailed_analysis()
        
        print("\n" + "="*70)
        print("ULTIMATE TRAINING COMPLETED!")
        print("="*70)
        print(f"Final Model Accuracy: {accuracy:.3f}")
        print("Ultimate model saved as 'soybean_ultimate_model.pkl'")
        print("Detailed analysis saved as 'detailed_analysis.png'")
        
        if accuracy >= 0.8:
            print("üéâ SUCCESS: 80%+ accuracy achieved!")
        else:
            print("‚ö†Ô∏è  Target not achieved. Consider collecting more real data.")

if __name__ == "__main__":
    main()